{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hYBr4TRgYl0",
        "outputId": "52c5b837-860a-4425-e20c-99a5fb4e4ac9"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain-community langchain-google-genai faiss-cpu sentence-transformers pypdf python-docx langchain-google-genai streamlit python-dotenv st-social-media-links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdZ1Aocngmed"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWli5zEFisI4"
      },
      "outputs": [],
      "source": [
        "!unzip /content/faiss_db.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro2ddseihR9l"
      },
      "source": [
        "# CODICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wC2ulTLgsXW",
        "outputId": "cdd292a3-facd-491c-fa17-4505625a6e10"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from st_social_media_links import SocialMediaIcons\n",
        "import time\n",
        "import os\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFaceHub, HuggingFaceEndpoint\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableBranch, RunnableParallel\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "\n",
        "st.set_page_config(page_title=\"Lucio chatbot\", page_icon=\":fox_face:\") #, layout=\"wide\"\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# VAR DI AMBIENTE E VAR GLOBALI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = \"Your_Gemini_Key\"\n",
        "\n",
        "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Your_HuggingFace_Key\"\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"Your_OpenAI_Key\"\n",
        "\n",
        "api_keys = {\n",
        "    \"Mistral\" : os.getenv('HUGGINGFACEHUB_API_TOKEN'),\n",
        "    \"Gemini\" : os.getenv('GOOGLE_API_KEY'),\n",
        "    \"GPT (coming soon...)\" : os.getenv('OPENAI_API_KEY')\n",
        "}\n",
        "\n",
        "chat_hist = [] # rende la char history disponibile alla chain dell'LLM\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# FUNZIONI\n",
        "\n",
        "def reset_chat_hist():\n",
        "    chat_hist = []\n",
        "    st.session_state.messages = []\n",
        "\n",
        "def extract_text(x):\n",
        "    documents = \"\"\n",
        "    sources = []\n",
        "    for doc in x['context']:\n",
        "        documents = documents + \"<chunk>\" + doc.page_content + \"</chunk>\\n\"\n",
        "        sources.append(doc.metadata[\"source\"])\n",
        "\n",
        "    sources = list(set(sources))\n",
        "    all_source = \", \".join(sources)\n",
        "\n",
        "    if show_sources == \"Visualizza i nomi documenti\":\n",
        "        with st.expander(\"Documenti utilizzati\"):\n",
        "          st.markdown(\"*I documenti utilizzati per rispondere sono: \"+all_source+\"*\")\n",
        "          st.write(\"\\n\")\n",
        "    elif show_sources == \"Visualizza il contenuto dei documenti\":\n",
        "        with st.expander(\"Documenti utilizzati\"):\n",
        "          st.markdown(\"*I chunks utilizzati per rispondere sono: \"+documents+\"*\")\n",
        "          st.write(\"\\n\")\n",
        "\n",
        "    return {\"context\": documents, \"question\": x['question']}\n",
        "\n",
        "def extract_history(x):\n",
        "    history = \"\"\n",
        "    if len(chat_hist)==0:\n",
        "      return history\n",
        "    for mess in chat_hist:\n",
        "        if mess[\"role\"]==\"Utente\":\n",
        "            history = history + \"Utente: \" + mess[\"content\"] + \"\\n\"\n",
        "        elif mess[\"role\"]==\"Lucio\":\n",
        "            history = history + \"Chatbot: \" + mess[\"content\"] + \"\\n\"\n",
        "    return history\n",
        "\n",
        "\n",
        "def response_generator(response):\n",
        "    for word in response.split():\n",
        "        yield word + \" \"\n",
        "        time.sleep(0.1)\n",
        "\n",
        "def create_llms(llm_name, temp_llm):\n",
        "    if llm_name == \"Gemini\":\n",
        "      llm_summarize = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, top_p=0.5)\n",
        "      llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=temp_llm, top_p=0.5)\n",
        "    elif llm_name == \"Mistral\":\n",
        "      llm_summarize = HuggingFaceEndpoint(repo_id ='mistralai/Mixtral-8x7B-Instruct-v0.1', temperature=0.3, token=api_keys[\"Mistral\"])\n",
        "      llm = HuggingFaceEndpoint(repo_id ='mistralai/Mixtral-8x7B-Instruct-v0.1', temperature=temp_llm, token=api_keys[\"Mistral\"])\n",
        "    elif llm_name == \"GPT (coming soon...)\":\n",
        "      llm_summarize = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3, openai_api_key=api_keys[\"GPT (coming soon...)\"])\n",
        "      llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=temp_llm, openai_api_key=api_keys[\"GPT (coming soon...)\"])\n",
        "\n",
        "    return llm_summarize, llm\n",
        "\n",
        "\n",
        "def create_prompts():\n",
        "    prompt_resumer_template='''\\\n",
        "    Riscrivi la domanda dell'utente riportata tra <q> e </q> \\\n",
        "    utilizzando lo storico della conversazione tra utente e chatbot \\\n",
        "    riportata tra <conv> e </conv>. Contestualizza la domanda dell'utente \\\n",
        "    usando lo storico, non devi riassumere la conversazione e non devi \\\n",
        "    menzionare il chabot.\n",
        "    <q>{question}</q>\n",
        "    <conv>{history}</conv>\n",
        "    '''\n",
        "    prompt_template = '''\\\n",
        "    Sei un chatbot che deve rispondere alle domande degli utenti riguardo \\\n",
        "    persone, aziende ed associazioni riportate nella documentazione.\n",
        "    La domanda dell'utente Ã¨ riportata tra <q> e </q>, mentre una lista di\n",
        "    estratti della documentazione sono riportati tra <ctx> e </ctx>, ogni \\\n",
        "    estratto Ã¨ separato da <chunk> e </chunk>. Se la domanda dell'utente \\\n",
        "    non riguarda un estratto della documetazione dici che non puoi rispondere.\n",
        "    <q>{question}</q>\n",
        "    <ctx>{context}</ctx>\n",
        "    '''\n",
        "    prompt_resumer = PromptTemplate.from_template(prompt_resumer_template)\n",
        "    prompt_llm = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    return prompt_resumer, prompt_llm\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# SIDEBAR\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Chi Ã¨ Lucio?\")\n",
        "\n",
        "\n",
        "    description = '''Lucio Ã¨ un chatbot italiano in grado di rispondere alle domande \\\n",
        "        degli utenti riguardante la documentazione della Camera dei Deputati italiana.\\\n",
        "            \\nA volte Ã¨ un po' timido, quindi siate gentili con lui!\\n\n",
        "    '''\n",
        "    st.markdown(description)\n",
        "    st.divider()\n",
        "\n",
        "    st.header(\"Settings\")\n",
        "    number_of_doc = st.number_input(\"Numero di documenti (RAG)\", min_value=1, max_value=30, value=5)\n",
        "    show_sources = st.selectbox(\"Impostazioni di visualizzazione RAG\", options=[\"Solo chat\",\"Visualizza i nomi documenti\",\"Visualizza il contenuto dei documenti\"], index=0)\n",
        "    st.subheader(\"Impostazioni LLM\")\n",
        "    llm_name = st.radio(\"Scegli il tuo LLM\", [\"Gemini\", \"Mistral\", \"GPT (coming soon...)\"], on_change=reset_chat_hist)\n",
        "    temp_llm = st.slider(\"Temperatura\", min_value=0.0, max_value=1.0, step=0.1, value=0.7)\n",
        "    c1,c2 = st.columns(2)\n",
        "    with c1:\n",
        "      if st.button(\"Reset pagina\"):\n",
        "          st.rerun()\n",
        "          reset_chat_hist()\n",
        "    with c2:\n",
        "      if st.button(\"Reset Lucio\"):\n",
        "          reset_chat_hist()\n",
        "\n",
        "    social_media_links = [\n",
        "      \"https://github.com/zurbojack/rag-chatbot-I3N\",\n",
        "      \"https://www.linkedin.com/company/langchain/\",\n",
        "      \"https://www.youtube.com/watch?v=G1IbRujko-A\",\n",
        "    ]\n",
        "\n",
        "    social_media_icons = SocialMediaIcons(social_media_links)\n",
        "    social_media_icons.render()\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# CARICAMENTO DATABASE E CREAZIONE LLM E RAG CHAIN\n",
        "\n",
        "with st.spinner(\"Svegliando Lucio...\"):\n",
        "\n",
        "    db_faiss = FAISS.load_local(\"./faiss_db/\", GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"), allow_dangerous_deserialization=True)\n",
        "    retriever = db_faiss.as_retriever(search_kwargs={\"k\": number_of_doc})\n",
        "\n",
        "    llm_summarize, llm = create_llms(llm_name, temp_llm)\n",
        "    prompt_resumer, prompt_llm = create_prompts()\n",
        "\n",
        "    def pritn(x):\n",
        "      st.write(x)\n",
        "      return x\n",
        "\n",
        "    q_summarize_chain =  RunnableBranch(\n",
        "        (lambda x: len(chat_hist)==1, lambda x: x),\n",
        "        lambda x: {\"history\": extract_history, \"question\": RunnablePassthrough()} | prompt_resumer | llm_summarize | StrOutputParser(),\n",
        "    )\n",
        "    retr_chain = RunnableParallel({\"context\": retriever, \"question\" : RunnablePassthrough()})\n",
        "    rag_chain = q_summarize_chain | retr_chain | extract_text | prompt_llm | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# SEZIONE PRINCIPALE\n",
        "\n",
        "st.title(\"Chatta con Lucio :speech_balloon:\")\n",
        "\n",
        "# prendo l'input dell'utente\n",
        "prompt = st.chat_input(\"Cosa ti interessa sapere?\")\n",
        "with st.container(height=650, border=False):\n",
        "\n",
        "    # Inizializzo chat history\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state[\"messages\"] = []\n",
        "\n",
        "    # Mostro la chat history passata\n",
        "    for message in st.session_state.messages:\n",
        "        if message[\"role\"] == \"Utente\":\n",
        "            with st.chat_message(message[\"role\"], avatar=\"ðŸ—£ï¸\"):\n",
        "                st.markdown(message[\"content\"])\n",
        "        else:\n",
        "            with st.chat_message(message[\"role\"], avatar=\"ðŸ¦Š\"):\n",
        "                st.markdown(message[\"content\"])\n",
        "\n",
        "    if prompt:\n",
        "        # Aggiungo il nuovo messaggio dell'utente alla history\n",
        "        st.session_state.messages.append({\"role\": \"Utente\", \"content\": prompt})\n",
        "        # Mostro i messaggi dell'utente nel container\n",
        "        with st.chat_message(\"Utente\", avatar=\"ðŸ—£ï¸\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        chat_hist = st.session_state.messages # serve momentaneamete per far vedere la history a extract_history nella chain\n",
        "        # Mostro i messaggi del chatbot nel container\n",
        "        with st.chat_message(\"Lucio\", avatar=\"ðŸ¦Š\"):\n",
        "            resp = rag_chain.invoke(prompt)\n",
        "            response = st.write_stream(response_generator(resp))\n",
        "        # Aggiungo il nuovo messaggio del chatbot alla history\n",
        "        st.session_state.messages.append({\"role\": \"Lucio\", \"content\": response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLg1dZBDhPsW"
      },
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g1K13yahNDw"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIcA2DlbhO2q",
        "outputId": "d603e17e-5957-4fdb-fdb0-6517ead37bb9"
      },
      "outputs": [],
      "source": [
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
