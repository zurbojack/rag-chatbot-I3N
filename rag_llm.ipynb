{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1aE4YqIpdS8"
      },
      "outputs": [],
      "source": [
        "!curl ipinfo.io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XyABRLAp3nL",
        "outputId": "e21a0e81-49b4-4962-c95c-1c016872de30"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain-community langchain-google-genai faiss-cpu sentence-transformers pypdf python-docx langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6janOsWLpRMz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pypdf import PdfReader\n",
        "from langchain.document_loaders import PyPDFLoader # alternativa nel reader\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nYTEVd82LOag"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = 'your-token-here'\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKiBeN5BqbsJ"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k68mXPHzyPvl"
      },
      "outputs": [],
      "source": [
        "!unzip /content/docx.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oRAicd5epRM5"
      },
      "outputs": [],
      "source": [
        "def get_gext(document):\n",
        "    full_text = []\n",
        "    for para in document.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return '\\n'.join(full_text)\n",
        "\n",
        "def split_into_paragraphs(text):\n",
        "    # Usa espressioni regolari per suddividere il testo in paragrafi\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "    # Rimuovi eventuali paragrafi vuoti\n",
        "    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
        "    return paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZCedk1QQLWeq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from docx import Document as word_doc\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "documents = []\n",
        "\n",
        "for doc_name in os.listdir(\"./docx/\"):\n",
        "    paragraphs_as_doc = []\n",
        "    if doc_name != \"pdfs\":\n",
        "        document = word_doc(\"./docx/\"+doc_name)\n",
        "        full_text = get_gext(document)\n",
        "        paragraphs = split_into_paragraphs(full_text)\n",
        "        for para in paragraphs:\n",
        "            paragraphs_as_doc.append(Document(page_content=para, metadata={\"source\": \"local\"}))\n",
        "\n",
        "    documents = documents + paragraphs_as_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PLhsMk7jLark"
      },
      "outputs": [],
      "source": [
        "db_faiss = None\n",
        "for i in range(0, len(documents), 50):\n",
        "  if db_faiss is None:\n",
        "    db_faiss = FAISS.from_documents(documents[i:i+50], gemini_embeddings)\n",
        "  else:\n",
        "    db_faiss.merge_from( FAISS.from_documents(documents[i:i+50], gemini_embeddings) )\n",
        "\n",
        "db_faiss.save_local('./faiss_db')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "NLfVtMnDLd-9"
      },
      "outputs": [],
      "source": [
        "retriever = db_faiss.as_retriever(search_kwargs={\"k\": 20})\n",
        "retrieved = retriever.invoke(\"dove ha la sede Assoproma ASD??\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OrZOF52LhWi"
      },
      "outputs": [],
      "source": [
        "[print(x.page_content + \"\\n\") for x in retrieved]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3oyz0_ipRM_"
      },
      "source": [
        "# LLM con chain personalizzata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ihZe_jSom1"
      },
      "outputs": [],
      "source": [
        "llm_mistral = HuggingFaceHub(\n",
        "    repo_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1', # tiiuae/falcon-7b-instruct\n",
        "    #model_kwargs={\"temperature\": temperature, \"max_length\": max_length},\n",
        "    huggingfacehub_api_token='your-token-here'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "XTe8MD2wset7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableBranch, RunnableParallel\n",
        "from langchain_core.output_parsers.string import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "xVRn_NNLpRM_"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, top_p=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "4JTFu2kmpRM_"
      },
      "outputs": [],
      "source": [
        "prompt_template = '''\\\n",
        "Rispondi alla domanda dell'utente riportata tra <q> e </q> \\\n",
        "utilizzando le informazioni riportate tra <ctx> e </ctx>.\n",
        "<q>{question}</q>\n",
        "<ctx>{context}</ctx>\n",
        "'''\n",
        "#e lo storico della conversazione (riportato tra <conv> e <\\conv>)\n",
        "#<conv>\n",
        "#{chat_history}\n",
        "#<\\conv>\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_JwCnXDtBlZ",
        "outputId": "5fc9ce9e-25cd-4203-a8d3-d172fe93fdd9"
      },
      "outputs": [],
      "source": [
        "def extract_text(x):\n",
        "  documents = \"\"\n",
        "  for doc in x['context']:\n",
        "    documents = documents + doc.page_content\n",
        "  return {\"context\": documents, \"question\": x['question']}\n",
        "\n",
        "retr_chain = RunnableParallel({\"context\": retriever, \"question\":RunnablePassthrough()})\n",
        "\n",
        "def print_p(x):\n",
        "  print(x)\n",
        "  return x\n",
        "\n",
        "rag_chain = retr_chain | extract_text | prompt | print_p | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "question = \"dove si trova Assoproma ASD?\"\n",
        "\n",
        "response = rag_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMTz3oHOx8nZ",
        "outputId": "17edd694-ff68-4835-b766-846bb5cf1ab7"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
